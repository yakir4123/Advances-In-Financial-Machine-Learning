{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 0 - Data Preparation\n",
    "Before I can start I need to get data of transactions. I'm using BTC-USDT trades at Binance Perpetual Future.\n",
    "The data downloaded from https://www.binance.com/en/landing/data .\n",
    "\n",
    "The avg monthly data size *compressed* is almost 900MB. \n",
    "The avg monthly data size *uncompressed* csv is 4.7GB. \n",
    "\n",
    "CSV files are heavy and slow to load to pandas so the first thing that im doing is to transform it to parquet so I can reuse it later much faster and less storage needed.\n",
    "\n",
    "The avg monthly parquet data size  is 1.3GB.\n"
   ],
   "id": "33537e4d849a6275"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-02T15:36:54.011423Z",
     "start_time": "2024-08-02T15:34:42.164278Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as TQDM\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def extract_rar(file_path: str, extract_to: str, pbar: TQDM) -> str | None:\n",
    "    try:\n",
    "        pbar.set_description(f\"Extracting: {file_path}\")\n",
    "        with ZipFile(file_path, 'r') as zObject:\n",
    "            zObject.extractall(extract_to)\n",
    "        pbar.set_description(f\"Extracted: {file_path}\")\n",
    "        return os.path.join(extract_to, zObject.namelist()[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed: {file_path}, {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_csv_with_columns(file_path):\n",
    "    \"\"\"\n",
    "    Some files don't have the columns' name in it. In case that it doesnt has the columns row ill add it manually\n",
    "    \"\"\"\n",
    "    first_row = pd.read_csv(file_path, nrows=1)\n",
    "    column_names = ['id', 'price', 'qty', 'quote_qty', 'time', 'is_buyer_maker']\n",
    "    if all(col in first_row.columns for col in column_names):\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        df.columns = column_names\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_csv_and_convert_to_parquet(csv_file_path: str, pbar: TQDM) -> str | None:\n",
    "    try:\n",
    "        pbar.set_description(f\"Read: {csv_file_path}\")\n",
    "        df = load_csv_with_columns(csv_file_path)\n",
    "\n",
    "        parquet_file_path = csv_file_path.replace('.csv', '.parquet')\n",
    "        pbar.set_description(f\"Save as {parquet_file_path}\")\n",
    "        df.to_parquet(parquet_file_path)\n",
    "        pbar.set_description(f\"Saved as {parquet_file_path}\")\n",
    "\n",
    "        return parquet_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {csv_file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def iterate_and_process(folder_path: str) -> None:\n",
    "    # Check if the folder path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Iterate over the files in the folder\n",
    "    root, dirs, files = next(iter(os.walk(folder_path)))\n",
    "    files = sorted(files)\n",
    "    files = [\n",
    "        '/Volumes/Extreme Pro/transactions/BTCUSDT-trades-2022-01.zip',\n",
    "        '/Volumes/Extreme Pro/transactions/BTCUSDT-trades-2023-01.zip',\n",
    "    ]\n",
    "    for file in (pbar := tqdm(files)):\n",
    "        if file.endswith('.zip'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            extracted_csv_path = extract_rar(file_path, folder_path, pbar)\n",
    "            # extracted_csv_path = file_path\n",
    "\n",
    "            if extracted_csv_path and extracted_csv_path.endswith('.csv'):\n",
    "                process_csv_and_convert_to_parquet(extracted_csv_path, pbar)\n",
    "\n",
    "\n",
    "transaction_path = r'/Volumes/Extreme Pro/transactions'\n",
    "iterate_and_process(transaction_path)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d649fcb8002745949725a52c7c505b37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-02T13:27:49.498143Z",
     "start_time": "2024-08-02T13:27:49.462057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import humanize\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_file_sizes(folder_path, file_type):\n",
    "    file_sizes = []\n",
    "\n",
    "    # Check if the folder path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return file_sizes\n",
    "\n",
    "    # Iterate over the files in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(file_type):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                file_sizes.append(file_size)\n",
    "\n",
    "    print(f'Mean {file_type} file size {humanize.naturalsize(sum(file_sizes) / len(file_sizes), binary=True)}')\n",
    "    print(f'Std {file_type} file size {humanize.naturalsize(np.std(file_sizes))}')\n",
    "    print(f'Total {file_type} file size {humanize.naturalsize(np.sum(file_sizes))}')\n",
    "\n",
    "\n",
    "# Specify the folder path\n",
    "transaction_path = r'E:\\transactions'\n",
    "print('Zip file sizes:')\n",
    "get_file_sizes(transaction_path, file_type='zip')\n",
    "print('CSV file sizes:')\n",
    "get_file_sizes(transaction_path, file_type='csv')\n",
    "print('Parquet file sizes:')\n",
    "get_file_sizes(transaction_path, file_type='parquet')"
   ],
   "id": "2a9d54c7509f3b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file sizes:\n",
      "The folder 'E:\\transactions' does not exist.\n",
      "CSV file sizes:\n",
      "The folder 'E:\\transactions' does not exist.\n",
      "Parquet file sizes:\n",
      "The folder 'E:\\transactions' does not exist.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next chapter, we work on bars generation. Its impossible to work with this size of data a local PC (mine is 64GB ram).\n",
    "So my bars generators are rolling, means that I iterate over the monthly transactions and create for each of them a bars (accepting the last bar transactions to roll the last bar)  "
   ],
   "id": "5f3383b2cf93c53d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
