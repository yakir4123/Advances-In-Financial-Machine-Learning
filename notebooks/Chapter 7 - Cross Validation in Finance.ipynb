{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b02f1a1ccc57f7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Part 2 - Modelling\n",
    "## Chapter 7 - Cross Validation in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c16c5d95ffbe5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.1 Why is shuffling a dataset before conducting k-fold CV generally a bad idea in finance? What is the purpose of shuffling? Why does shuffling defeat the purpose of k-fold CV in financial datasets?\n",
    "\n",
    "Shuffling is bad idea because of data leakage and non IID samples, if we shuffle the data we cant monitor if there is a leakage from our training to the testing model. makes the k fold irrelevant because each test set has similar samples in training set and it may lead to false features discoveries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c09139ad259522",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.2 Take a pair of matrices (X, y), representing observed features and labels. These could be one of the datasets derived from the exercises in Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20b4a06be69bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:49:00.465995Z",
     "start_time": "2024-08-25T14:48:56.442397Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from afml.modelling.ensamble_methods import RandomForestClassifier\n",
    "from afml.data_analyst.financial_data_structures import get_t_events\n",
    "from afml.data_analyst.labels import get_vertical_next_day, get_events_triple_barrier, get_bins, get_daily_vol\n",
    "from afml.data_analyst.fractionally_differentiated_features import frac_diff_ffd\n",
    "\n",
    "btc_dollar = pd.read_parquet('../data/dollar-bars-100000000-True.parquet')\n",
    "btc_dollar = btc_dollar[btc_dollar.index > pd.Timestamp('2023-01-01').timestamp() * 1000]\n",
    "\n",
    "cum_log_prices = np.log(btc_dollar['close']).cumsum()\n",
    "fracdiff_series = frac_diff_ffd(pd.DataFrame(np.log(btc_dollar['close']).cumsum()), 2, tau=1e-5)\n",
    "t0 = get_t_events(fracdiff_series.index.values, fracdiff_series['close'].values,\n",
    "                  float(4 * np.std(fracdiff_series.values)))\n",
    "\n",
    "daily_vol = get_daily_vol(btc_dollar)\n",
    "ms_a_day = 24 * 60 * 60 * 1000\n",
    "num_days = 5\n",
    "t1 = get_vertical_next_day(btc_dollar, t0, num_days * ms_a_day)\n",
    "\n",
    "events = get_events_triple_barrier(\n",
    "    close=btc_dollar.loc[fracdiff_series.index, 'close'],\n",
    "    t0=t0,\n",
    "    tp_scale=2,\n",
    "    sl_scale=2,\n",
    "    target=daily_vol,\n",
    "    min_return=0,\n",
    "    t1=t1,\n",
    ")\n",
    "labels = get_bins(events, btc_dollar['close'], t1)\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'close': fracdiff_series['close'].loc[labels.index],\n",
    "    'close_lag_1': fracdiff_series['close'].shift(1).loc[labels.index],\n",
    "    'close_lag_2': fracdiff_series['close'].shift(2).loc[labels.index],\n",
    "},\n",
    "    index=labels.index\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae558e9cd6c8f9",
   "metadata": {},
   "source": [
    "#### 7.2 (a) Derive the performance from a 10-fold CV of an RF classifier on (X, y), without shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fed11346cc1191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:49:03.000372Z",
     "start_time": "2024-08-25T14:49:00.465820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4878676470588236)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "kf = KFold(n_splits=10, shuffle=False)  # 10-fold CV without shuffling\n",
    "\n",
    "for train_index, test_index in kf.split(X.index):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = labels['bin'].iloc[train_index], labels['bin'].iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the performance metric\n",
    "    score = accuracy_score(y_test, y_pred)  # Replace with your metric\n",
    "    scores.append(score)\n",
    "\n",
    "# Average performance across the 10 folds\n",
    "mean_performance = np.mean(scores)\n",
    "mean_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1965452d434b6",
   "metadata": {},
   "source": [
    "#### 7.2 (b) Derive the performance from a 10-fold CV of an RF on (X, y), with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:49:31.008999Z",
     "start_time": "2024-08-25T14:49:28.344609Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4885152345251826)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "kf = KFold(n_splits=10, shuffle=True)  # 10-fold CV without shuffling\n",
    "\n",
    "for train_index, test_index in kf.split(X.index):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = labels['bin'].iloc[train_index], labels['bin'].iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the performance metric\n",
    "    score = accuracy_score(y_test, y_pred)  # Replace with your metric\n",
    "    scores.append(score)\n",
    "\n",
    "# Average performance across the 10 folds\n",
    "mean_performance = np.mean(scores)\n",
    "mean_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c635d76569602",
   "metadata": {},
   "source": [
    "#### 7.2 (c) Why are both results so different?\n",
    "The results are quite similar because I barely use any features! But the results when the data is shuffled are better because there is high leakage means that the test data also \"exist\" in the test data, these leakages are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7e9803fb3fa94",
   "metadata": {},
   "source": [
    "#### 7.2 (d) How does shuffling leak information?\n",
    "Because the data samples is not IID, each sample take since we sample it till we decide a label it takes T tick, it's easier to be overlaps between samples between the train set to the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d415ab8b9b13e3",
   "metadata": {},
   "source": [
    "### 7.3 Take the same pair of matrices (X, y) you used in exercise 2.\n",
    "#### 7.3 (a) Derive the performance from a 10-fold purged CV of an RF on (X, y), with 1% embargo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86c6073cc017d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T22:22:12.855836Z",
     "start_time": "2024-08-25T22:22:09.993570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4885416666666666)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from afml.modelling import cross_validation\n",
    "\n",
    "importlib.reload(cross_validation)\n",
    "from afml.modelling.cross_validation import PurgedKFold\n",
    "\n",
    "scores = []\n",
    "kf = PurgedKFold(n_splits=10, t1=t1, pct_embargo=0.01)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = labels['bin'].iloc[train_index], labels['bin'].iloc[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation fold\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store the performance metric\n",
    "    score = accuracy_score(y_test, y_pred)  # Replace with your metric\n",
    "    scores.append(score)\n",
    "\n",
    "# Average performance across the 10 folds\n",
    "mean_performance = np.mean(scores)\n",
    "mean_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0efc1cb13d5ec",
   "metadata": {},
   "source": [
    "#### 7.3 (b) Why is the performance lower?\n",
    "Again, there is not data so I cant get any result that worth something, but the reason why this code should give a lower result is because there is less data leakage, thanks to the embargo and the purged data, the samples are not leake from trainding / testing to the testing / training and there is less correlation between the neighbors samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f99d0a892db8b",
   "metadata": {},
   "source": [
    "#### 7.3 (c) Why is this result more realistic?\n",
    "the test sets is without samples that has more probability to be correct because of unrealized correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238044b7b6fe3160",
   "metadata": {},
   "source": [
    "### 7.4 In this chapter we have focused on one reason why k-fold CV fails in financial applications, namely the fact that some information from the testing set leaks into the training set. Can you think of a second reason for CV’s failure?\n",
    "\n",
    "Because financial data is non-stationary and high correlated in time, than even though we use purged and embargo tools, we may see a good result with k-fold CV, but it may be because $S_{k_i}$ and $S_{k_{i+2}}$ that is in the training set are correlated with similar market conditions to the test set $S_{k_{i+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa8d50dd4672dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.5 Suppose you try one thousand configurations of the same investment strategy, and perform a CV on each of them. Some results are guaranteed to look good, just by sheer luck. If you only publish those positive results, and hide the rest, your audience will not be able to deduce that these results are false positives, a statistical fluke. This phenomenon is called “selection bias.”\n",
    "#### 7.5 (a) Can you imagine one procedure to prevent this?\n",
    "1. Divide the data to train and test, the cross-validation run only on the train set, and the test set is used only for the final test.\n",
    "2. Run cross-validation with different k-size to see if you get similar results.\n",
    "3. Sometimes overfit can be determined in case we change the parameters just a little and the results change dramatically. publish results with different closer parameters may help to see if the results are stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950a73093f43e23",
   "metadata": {},
   "source": [
    "\n",
    "#### 7.5 (b) What if we split the dataset in three sets: training, validation, and testing? The validation set is used to evaluate the trained parameters, and the testing is run only on the one configuration chosen in the validation phase. In what case does this procedure still fail?\n",
    "If failing means the CV shows a good result, but the testing shows bad results, it probably means overfit.\n",
    "In case failing means shows a good result but the model may fail in the future it might be because of non-stationary process and regime changed and the model is not predicted correct anymore.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a5758212030d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 7.5 (c) What is the key to avoiding selection bias?\n",
    "* Cross-validation with small K\n",
    "* Show results with different configurations\n",
    "* Show results with similar configurations\n",
    "* Check the results on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
